{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini- Project 1 Notebook\n",
    "\n",
    "The purpose of this mini project is to be able to implement a regressor that can estimate with a reasonable mean squared error, the popularity of a reddit commnet, given it's controversiality, whether or not that comment is a root comment, the number of children it has. Furthermore, apart from these features, we are meant to implement text features and extend those by ourselves.\n",
    "\n",
    "For this notebook to work, make sure the file \"proj1_data_loading.py\" is in your current directory, and that you have the other modules specified below. This notebook will outline without showing the nitty gritty how our code works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import proj1_data_loading as p1\n",
    "import json \n",
    "import collections\n",
    "import numpy as np\n",
    "import argparse\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "redcoms = p1.RedditComments()\n",
    "redcoms.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redcoms just loads our own handcoded RedditComments class. The class contains many methods useful for manipulating, preprocessing, and running regression on our dataset. It also has one attribute, data that contains well, all the data in the json file. \n",
    "\n",
    "redcoms.load() simply sets the data attribute to the contents of the json file.\n",
    "\n",
    "Below, we will set a couple of pre-processing parameters and hyper-parameters for our model and data. The hyper-parameters are classic for gradient descent, where n0 specifies the starting value of alpha.\n",
    "\n",
    "Here is a description of what each boolean  does:\n",
    "\n",
    "## Parameter Descriptions \n",
    "#### closed_form :\n",
    "if set to True, our model will run the closed form approach to linear regression. This model always finds the best training_mse, but can often be costly to compute for huge matrices.\n",
    "\n",
    "#### text_features: \n",
    "if set to True, will preprocess the data with text_features, appending a matrix the size of (NUM_INSTANCES * most_common) to the initial training data. This array describes the commonalities of the comments... that is to say we would first find the N most common words in all of the comments, then go through each comment and check if any of the N words are present in that comment, and if so how many. It then appends the number of instances of that word to the text_features matrix, at the index i,j... where i is the instance_number and j is the index of the word, determined by how common it is throughout the dataset.\n",
    "This boolean feature must be run with a most_common feature greater than 0.\n",
    "\n",
    "#### most_common:\n",
    "Integer number specifying how many of the most common words we will be looking at. Must be used with either text_features or binary_features set to True.\n",
    "\n",
    "#### swear_words:\n",
    "If set to True, will go through each comment and check how many swear words it contains. Will append that array to our data.\n",
    "\n",
    "#### stop_words:\n",
    "If set to True, will disqualify any stop words such as \"The, a, I\" to enter into the most common text_features\n",
    "\n",
    "#### discrete_swear_words:\n",
    "Much like swear words, except binarized: If the comment contains any swear word at all, it takes a value of one. If not, 0.\n",
    "\n",
    "#### comment_lenght:\n",
    "If set to True, will go through each comment, count it's lenght, and if the length is greater than 6, gives that comment a 1. If not, it gives it a zero.\n",
    "\n",
    "#### binary_text:\n",
    "Alot like text features, but instead of counting how many times each most common word appears in each comment, just gives a one if it appears in that comment at all... and a 0 if it doesn't\n",
    "\n",
    "#### k_fold_cross_validation:\n",
    "If set to true, will run cross_validation. We use this to test our models further and check if they are overfitting or not. Must be run with num_folds set to a number. Preferably 10, as is standard.\n",
    "\n",
    "#### num_folds:\n",
    "Integer number of folds to split the 11000 train/split array of data. We recommend 10. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD Hyperparameters\n",
    "args = {\n",
    "    'n0':1e-5,\n",
    "    'beta':1e-4,\n",
    "    'epsilon':1e-6,\n",
    "    'maxiters':10\n",
    "     }\n",
    "    ###################################################################################\n",
    "\n",
    "#should we run SGD or closed form regression?\n",
    "closed_form = False\n",
    "\n",
    "#preprocessing data features you want\n",
    "text_features = False\n",
    "most_common = 10\n",
    "swear_words =False\n",
    "stop_words =  False\n",
    "discrete_swear_words = False\n",
    "comment_length = False\n",
    "binary_text = False\n",
    "    \n",
    "    #cross validation boolean\n",
    "k_fold_cross_validation = True\n",
    "num_folds = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Now for actually running the preprocessing... \n",
    "First, with the non-cross validated case: simply call our preprocess_features in the RedditComments class, with all the booleans appended as kwargs. Make sure you define how split up the data in how you index the data matrix, like below. Our preprocess method takes care of all the preprocessing features mentioned above that you desire.\n",
    "\n",
    "Here, we can see what X_val looks like. If all parameters are false, it should just have 4 columns... the first for the bias term, and the last three for the three base text_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not k_fold_cross_validation):\n",
    "    # Preprocess text, inputs into integers to be formatted into data matrix\n",
    "    text_train, X_train, y_train = redcoms.preprocess_features(redcoms.data[:10000], text_features=text_features, most_common=most_common, stop_words=stop_words, swear_words=swear_words, discrete_swear_words=discrete_swear_words, comment_length=comment_length, binary_text=binary_text)\n",
    "    text_val, X_val, y_val = redcoms.preprocess_features(redcoms.data[10000:11000], text_features=text_features, most_common=most_common, stop_words=stop_words,  swear_words=swear_words, discrete_swear_words=discrete_swear_words, comment_length=comment_length, binary_text=binary_text)\n",
    "    text_test, X_test, y_test = redcoms.preprocess_features(redcoms.data[11000:12000], text_features=text_features, most_common=most_common,stop_words=stop_words,  swear_words=swear_words, discrete_swear_words=discrete_swear_words, comment_length=comment_length, binary_text=binary_text)\n",
    "\n",
    "    print(X_val)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "Now, for actually running regression on the preprocessed data... simply call the regress method in the RedditComments class. Give it as input the boolean of whether you want a closed form or SGD solution, as well as the training data you want, and the text_train and args parameter in case it runs stochastic gradient descent. It outputs the training weights, the training MSE, and the training time. \n",
    "\n",
    "Once you have that, you can take those weights and input them into the get_mse method of our class like shown below. It takes as input X and y data that you wish to try out these weights on, as well as the weights you want to try. It outputs the error and a squared error array, which specifies the Squared error for each entry in Y and the prediction of Y according to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD training time:   0.3548698425292969\n",
      "SGD training mean squared error: 1.1306808500346766\n",
      "SGD validation mean squared error:    [1.07344077]\n"
     ]
    }
   ],
   "source": [
    "if not k_fold_cross_validation:\n",
    "    training_weights, training_error, training_time = redcoms.regress( closed_form, X_train, y_train, text_train, args )    \n",
    "    validation_error, squared_error_array = redcoms.get_mse(X_val, y_val, training_weights)\n",
    "    if closed_form:\n",
    "        print(\"Closed form training time:   \" + str(training_time))\n",
    "        print(\"Closed form training mean squared error: \" +  str(training_error[0]))\n",
    "        print(\"closed form validation mean squared error:    \" + str(validation_error));\n",
    "    else:\n",
    "        print(\"SGD training time:   \" + str(training_time))\n",
    "        print(\"SGD training mean squared error: \" +  str(training_error[0]))\n",
    "        print(\"SGD validation mean squared error:    \" + str(validation_error));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "Notice how the last bit of code was in an if statement based on the cross_validation boolean parameter? If that was set to true, then you'd skip the code above and come straight here.\n",
    "\n",
    "To preprocess the data is a little different... you need to pick how many values you want to be in your test set by indexing properly... then the rest will be split according to the k fold algorithm. We still use the method preprocess_features, just like above though, if you'll notice. \n",
    "\n",
    "Furhtermore for the sake of testing our model and implementation on different data, we chose the test set to be the FIRST 1000 features, not the last like it was done above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-8988385ff0a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mredcoms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mredcoms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmost_common\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswear_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswear_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscrete_swear_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdiscrete_swear_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomment_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomment_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtext_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mredcoms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mredcoms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmost_common\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswear_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswear_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscrete_swear_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdiscrete_swear_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomment_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomment_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Desktop\\COMP551\\Assignments\\A1\\proj1_data_loading.py\u001b[0m in \u001b[0;36mpreprocess_features\u001b[1;34m(self, data, **kwargs)\u001b[0m\n\u001b[0;32m    635\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m                 \u001b[0minstance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    638\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m                 \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTweetTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "if (k_fold_cross_validation):\n",
    "    text, X, y = redcoms.preprocess_features(redcoms.data[1000:], text_features=text_features, most_common=most_common, stop_words=stop_words, swear_words=swear_words, discrete_swear_words=discrete_swear_words, comment_length=comment_length, binary_text=binary_text)\n",
    "    text_test, X_test, y_test = redcoms.preprocess_features(redcoms.data[:1000], text_features=text_features, most_common=most_common, stop_words=stop_words, swear_words=swear_words, discrete_swear_words=discrete_swear_words, comment_length=comment_length, binary_text=binary_text)\n",
    "    \n",
    "    print(X)\n",
    "#first 1000 for testing, last 1100 for train/valid split.\n",
    "        \n",
    "    foldnum = num_folds\n",
    "    seg_length = len(X)/num_folds\n",
    "       \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the regression step, where it gets a little complicated. We first determine the validation data. Once we have that we  our training data to everything that's left.  We then take that training data, use the regress_crossv() method in our RedditComments class, and input the initial weights along with our other old inputs from the original regress() method. \n",
    "\n",
    "Once the regression is complete, we use those weights to check out how it performs on the validation data. Finally we append both training and validation mse's an array. \n",
    "\n",
    "Once that iteration is done, we start another. We slide the validation data over, and make the training data equal to what's rest. In regression_crossv, we input the previous iterations determined training weights. We append our results and keep repeating until the validation set has reached the end of what's available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f7ac93e79d89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtempX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtempy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m    \u001b[1;31m#starting weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtraining_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmean_train_mse_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "if (k_fold_cross_validation):\n",
    "    tempX = np.copy( X)\n",
    "    tempy = np.copy(y)\n",
    "       #starting weights\n",
    "    training_weights = np.zeros([X.shape[1], 1])\n",
    "    mean_train_mse_arr = np.array([])\n",
    "    mean_valid_mse_arr = np.array([])\n",
    "    for i in range(foldnum-1):\n",
    "           \n",
    "        seg_length = int(seg_length)\n",
    "        X_val = X[(i) * seg_length: (i+1)*seg_length]\n",
    "        y_val = y[(i) * seg_length: (i+1)*seg_length]\n",
    "        X_train = np.delete(tempX, [range((i) * seg_length,  (i+1)*seg_length)], axis=0)\n",
    "        y_train = np.delete(tempy, [range((i) * seg_length,  (i+1)*seg_length)], axis=0)\n",
    "           \n",
    "           #get training weights, input last iteration's training weights. \n",
    "        training_weights, training_error, training_time = redcoms.regress_crossv( closed_form, X_train, y_train, training_weights, text, args )    \n",
    "         #  training_weights = training_weights[:, 0]\n",
    "        valid, squared_error_array = redcoms.get_mse_crossv(X_val, y_val, training_weights)\n",
    "        mean_train_mse_arr = np.append( mean_train_mse_arr, training_error)\n",
    "        mean_valid_mse_arr = np.append(mean_valid_mse_arr, valid)\n",
    "         #  test_error, square_error_array = redcoms.get_mse(X_test, y_test, training_weights)\n",
    "        print(\"Fold : \" + str(i+1) + \"\\nTraining MSE: \\t\" + str(training_error) + \"\\nValidation MSE:\\t : \" + str(valid))\n",
    "       \n",
    "       \n",
    "    print (\"\\nmean of training: \" + str(np.mean(mean_train_mse_arr)))\n",
    "    print(\"mean of valid :    \" + str(np.mean(mean_valid_mse_arr)))\n",
    "       \n",
    "       \n",
    "    test_err, squared_error_array = redcoms.get_mse_crossv(X_val, y_val, training_weights)\n",
    "    print (\"testing MSE:  \" + str(np.mean(test_err)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
